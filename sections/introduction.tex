% !TEX root = main.tex

\section{Introduction}

Alternative interaction methods are increasingly common in modern computing systems. Devices like smartphones, wearables~\cite{tobii_glasses_x}, and headsets~\cite{apple_vision_pro_2025,playstation_vr2_specs,vive_pro2_2025} incorporate novel input technologies that enable more natural, inclusive, and adaptive user experiences~\cite{dondi2023gazehci, fernandes2023eyevr}. These innovations are especially valuable for accessibility, offering alternative ways for users with physical limitations to engage with digital content~\cite{hsieh2024increasing}.

While eye-tracking has long been studied, its integration into consumer devices is making it a practical input method, enabling precise, intuitive, and hands-free interaction~\cite{huang2024visionpro}. In contrast, NLP is already a well-established modality, powering spoken interfaces from voice-enabled coding tools~\cite{serenade2025} to smart assistants. However, these systems still face challenges in accurately interpreting user intent, underscoring the need for robust semantic and contextual processing~\cite{mozafari2020chatbot, liu2024chatgpt}.

EyeNav introduces a novel multimodal input approach by combining real-time eye-tracking and natural language commands for web interaction. Implemented as a Chrome extension, it supports gaze-based navigation, voice commands, and automated testing via a Gherkin-based record-and-replay module. Originally designed for users with motor impairments, EyeNav also benefits developers seeking hands-free browser control, usability professionals conducting accessibility evaluations, and users exploring multimodal interaction. By merging assistive input with test automation, EyeNav advances the integration of accessible technologies into mainstream web environments.%