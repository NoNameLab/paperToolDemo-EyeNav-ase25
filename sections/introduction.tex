% !TEX root = main.tex

\section{Introduction}

Alternative interaction methods are becoming increasingly prevalent across modern computing systems. 
Devices such as smartphones and tablets~\cite{apple2024accessibility, honor_magic6pro_specs}, wearables\cite{tobii_glasses_x}, and headsets\cite{apple_vision_pro_2025,playstation_vr2_specs,vive_pro2_2025} frequently incorporate novel input technologies, enabling more natural, inclusive, and adaptive user experiences~\cite{dondi2023gazehci, fernandes2023eyevr}.
These emerging methods hold particular significance for accessibility applications, as they provide alternative means for users with motor impairments or other physical limitations to interact with digital content~\cite{hsieh2024increasing}.

Although eye-tracking has been the subject of research for many years\cite{gips1996eagleeyes}, it is increasingly emerging as an input modality through its growing integration into everyday consumer devices. When implemented effectively, it enables precise, intuitive, and hands-free control~\cite{huang2024visionpro}.

On the contrary, \ac{NLP} is already well establised as a compelling input method, enabling users to interact with software through spoken commands. 
From voice-enabled code generation~\cite{serenade2025} to smart assistants, speech interfaces are widely researched and deployed. Still, these systems can become frustrating if user intent is misinterpreted or ignored, highlighting the importance of robust semantic parsing and contextual understanding~\cite{mozafari2020chatbot, liu2024chatgpt}. 

EyeNav is an early prototype developed to introduce a novel input method, integrating real-time eye-tracking with natural language interaction within web applications. This system facilitates interactions that are intuitive, accessible, and innovative.

Implemented as a Chrome extension, EyeNav allows users to interact with pages via gaze-based control and spoken commands. It also integrates a record-and-replay module to support automated testing workflows.

Initially designed for users with motor impairments; it also holds potential for developers interested in hands-free browser control or rapid prototyping, usability professionals conducting accessibility evaluations in web environments, and general users exploring novel multimodal web interactions. By bridging assistive technologies and test automation, the prototype facilitates broader evaluation and integration of accessibility focused solutions within mainstream web contexts.

This paper outlines our approach overview, methodology, evaluation and discusses implementation and future work considerations for the system.
